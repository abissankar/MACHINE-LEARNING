{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPOHClSA+jR8RC2ujuRCIU6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tIhago330EWb"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","import tensorflow as tf\n","from torch.utils.data import DataLoader\n","from tensorflow.keras import layers,models\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n"]},{"cell_type":"code","source":["# Load and preprocess the MNIST dataset using PyTorch\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n","test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)"],"metadata":{"id":"DHeC2bH81OYM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"],"metadata":{"id":"0-C8nzYB1mZT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"],"metadata":{"id":"9qA0dPcL2NyE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n","test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255"],"metadata":{"id":"R5Axfc8U2-8m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_labels = to_categorical(train_labels)\n","test_labels = to_categorical(test_labels)"],"metadata":{"id":"gpYvLvnl3Eby"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the neural network using PyTorch\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(28 * 28, 128)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(128, 10)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.softmax(x)\n","        return x\n"],"metadata":{"id":"vaOQgOtf3JDv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_pt = Net()\n","criterion = nn.CrossEntropyLoss()\n","optimizer_pt = optim.Adam(model_pt.parameters(), lr=0.001)"],"metadata":{"id":"8ghkTtNd3UJP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","start_time_tf = time.time()\n","model_tf = models.Sequential()\n","model_tf.add(layers.Flatten(input_shape=(28, 28, 1)))\n","model_tf.add(layers.Dense(128, activation='relu'))\n","model_tf.add(layers.Dense(10, activation='softmax'))"],"metadata":{"id":"qHXK12tV34l8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_tf.compile(optimizer= 'adam'  ,\n","                 loss='categorical_crossentropy',\n","                 metrics=['accuracy'])"],"metadata":{"id":"wKpiydGN37tV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_tf.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n","end_time_tf = time.time()"],"metadata":{"id":"3sINz6lQ4W-A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"TensorFlow Training Time: {end_time_tf - start_time_tf} seconds\")"],"metadata":{"id":"3yve2u4J4aUA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start_time_pt = time.time()\n","for epoch in range(5):\n","    for images, labels in train_loader:\n","        optimizer_pt.zero_grad()\n","        outputs = model_pt(images.view(-1, 28 * 28))\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer_pt.step()\n","end_time_pt = time.time()\n","print(f\"PyTorch Training Time: {end_time_pt - start_time_pt} seconds\")"],"metadata":{"id":"D7FY6yuy4sS8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluation using TensorFlow\n","test_loss_tf, test_acc_tf = model_tf.evaluate(test_images, test_labels)\n","print(f\"TensorFlow Test Accuracy: {test_acc_tf}\")"],"metadata":{"id":"rDj3OX_44vsM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluation using PyTorch\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        outputs = model_pt(images.view(-1, 28 * 28))\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy_pt = correct / total\n","print(f\"PyTorch Test Accuracy: {accuracy_pt}\")"],"metadata":{"id":"OdV0ajT940SK"},"execution_count":null,"outputs":[]}]}